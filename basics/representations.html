
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Representing Audio &#8212; Open-Source Tools &amp; Data for Music Source Separation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Open-Source Tools & Data for Music Source Separation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Tutorial for the Music Demixing Challenge @ ISMIR 2021
  </a>
 </li>
</ul>
    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/basics/representations.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/source-separation/tutorial"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/source-separation/tutorial/issues/new?title=Issue%20on%20page%20%2Fbasics/representations.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#waveforms">
   Waveforms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#desirable-properties-of-representations">
   Desirable Properties of Representations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#input-representations">
   Input Representations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#time-frequency-tf-representations">
     Time-Frequency (TF) Representations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#short-time-fourier-transform-stft">
       Short-time Fourier Transform (STFT)
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#window-types">
         Window Types
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#window-length">
         Window Length
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#hop-length">
         Hop Length
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#overlap-add">
         Overlap-Add
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#magnitude-power-and-log-spectrograms">
       Magnitude, Power, and Log Spectrograms
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mel-spaced-spectrograms">
       Mel-spaced Spectrograms
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-representations">
     Other Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#output-representations">
   Output Representations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#which-is-better-inputting-a-waveform-or-a-time-frequency-representation">
   Which is Better? Inputting a Waveform or a Time-Frequency Representation?
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="representing-audio">
<span id="representingaudio"></span><h1>Representing Audio<a class="headerlink" href="#representing-audio" title="Permalink to this headline">¶</a></h1>
<p>The first thing we want to examine are the input and output representations of a source
separation system and how the inputs and outputs are represented. In its most
unprocessed form, we assume that audio is stored as a waveform. Some source
separation approaches operate on the waveform directly, although many require
some preprocessing before separating sources. In this section, we will discuss
the different types of input and output representations that are commonly used
in source separation approaches.</p>
<p>We’ll first start talking about waveforms, arguably the most fundamental representation
of audio, then we’ll talk about what types of things are useful when we represent
audio for source separation before we move onto some common audio representations
for source separation.</p>
<div class="section" id="waveforms">
<h2>Waveforms<a class="headerlink" href="#waveforms" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="waveform-gif">
<img alt="Waveform shown at many different time scales from a few seconds to a few samples." src="../_images/richter.gif" />
<p class="caption"><span class="caption-text">A waveform shown at many different time scales. Each value is sampled at a uniform
rate and quantized. Image used courtesy of Jan Van Balen (<a href="https://jvbalen.github.io/notes/waveform.html">source</a>).</span><a class="headerlink" href="#waveform-gif" title="Permalink to this image">¶</a></p>
</div>
<p>A <em>waveform</em> is shorthand for a digitized audio signal, which is most similar to
what the sound is like physically. For an acoustic sound, the air pressure of
over time changes, and is
recorded by a microphone, which converts the changes in air pressure to an electrical
signal. The voltage of this signal is sampled at a regular time interval,
quantized, and converted to a digital array in a computer. This digital array is
what we call the waveform. Of course, this description glosses over a lot of
details in the realm of physics, acoustics, and signal processing. What’s
important to know is that a continuous-time signal is discretized in both time
and amplitude. We say a signal is monophonic, or mono, if there
is only one audio channel, i.e., this array has shape <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{t \times 1}\)</span>.
We say a signal is stereophonic, or stereo, if the array has two
channels, i.e., this array has shape <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{t \times 2}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Audio signals with more than 2 channels have many applications (e.g.
5.1 surround sound), including applications for in separating sources.
Approaches that input many audio channels for separation are typically refered
to under the title of <em>beamforming</em> approaches. Beamforming is a related, but
separate area of active research. As such, the work we are going to cover in
this space is sometimes called <em>Single Channel Source Separation</em>.</p>
</div>
<p>An important aspect of the waveform is the sample rate, which describes how
many measurements, or samples, happen per second and is measured in Hertz, or
Hz<a class="footnote-reference brackets" href="#fn2" id="id1">1</a>. For a signal with sample rate <span class="math notranslate nohighlight">\(sr\)</span>, the maximum frequency that can be
reliably represented is <span class="math notranslate nohighlight">\(f_N=\frac{sr}{2}\)</span>, which is called the
<a class="reference external" href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist frequency</a>.
For example, if a signal has a sample rate of 44.1 kHz, the highest
possible frequency is 22.05 kHz.</p>
<p>Many deep learning-based source separation approaches will reduce the sample
rate of their input signals (called downsampling) to reduce the
computational load during training time. Downsampling removes high frequency
information from a signal, which is seen as a necessary evil when prototyping
models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All of the source separation approaches we will discuss assume that the sample
rate between the training, validation, and testing data is the same. The
assumptions that ensure the approaches work are violated if the sample rate is
variable. For example, if a system expects a signal at 16 kHz, then
all input audio should be resampled to 16 kHz before using it.</p>
</div>
</div>
<div class="section" id="desirable-properties-of-representations">
<h2>Desirable Properties of Representations<a class="headerlink" href="#desirable-properties-of-representations" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="representation-img">
<img alt="A signal that looks completely unseparable in one representation becomes easily separable in another representation." src="../_images/right_representation.png" />
<p class="caption"><span class="caption-text">Examples of two signals represented differently. The first row has two ways of
displaying a mixture of two single-frequency sine waves; panel (a) represents
the waveform and panel (b) shows the frequency spectrum. The second row has two
ways of displaying a mixture of two linear chirp signals (<em>i.e.</em>, sine waves
with rising frequency); panel (c) shows a frequency spectrum and panel (d) shows
a time-frequency representation. Notice how a signal that looks completely
unseparable in one representation becomes easily separable in another representation.
Image used courtesy of Fatemeh Pishdadian and Bryan Pardo. <span id="id2">[<a class="reference internal" href="../zzz_refs.html#id7">PP18</a>]</span></span><a class="headerlink" href="#representation-img" title="Permalink to this image">¶</a></p>
</div>
<p>It can be argued that a source separation approach is only as good as its ability
to represent audio in a separable manner. With that in mind, it’s important
to understand how audio itself is represented for the purposes of source separation.
For the source separation approaches we will explore here, we will see many
variations on the same theme, namely:</p>
<ol class="simple">
<li><p>Convert the audio to a representation easy to separate</p></li>
<li><p>Separate the audio by manipulating this representation</p></li>
<li><p>Convert the audio back from the manipulated representation to get isolated sources.</p></li>
</ol>
<p>Almost every source separation approach we discuss here–classic and deep–can
be broken down into these three steps. We want to note that each of these three
steps might in fact involve multiple separate substeps.</p>
<p>Therefore an important aspect of an audio representation is invertability, or
whether a signal that is converted from a waveform to a new representation can
reliably be converted back to a waveform with little-to-no error. Artifacts that
arise from converting back and forth will be audible in our separation output, so
we want to minimize these types of errors (although eliminating them does not
guarantee a perfect separation).</p>
<p>An other important aspect is whether this representation can keep data from
one source apart from another source. As seen in <code class="xref std std-numref docutils literal notranslate"><span class="pre">representation-img</span></code>,
some representations might be better suited to separate certain signals than other
representations. In this tutorial we will explore methods that produce representations
specifically designed for separating sources.</p>
<p>Some recent source separations approaches use deep learning to learn a representation
directly from the waveform, while others use preprocessing tools that are common
in the audio signal processing and music information retrieval literature as a first
step.</p>
<p>We will first discuss time frequency representations because they are
an important preprocessing step, the parameters of which could have an impact
on the separation quality of your approach.</p>
</div>
<div class="section" id="input-representations">
<h2>Input Representations<a class="headerlink" href="#input-representations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="time-frequency-tf-representations">
<h3>Time-Frequency (TF) Representations<a class="headerlink" href="#time-frequency-tf-representations" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="tf-repr">
<img alt="A time-frequency representation." src="../_images/tf_representation.png" />
<p class="caption"><span class="caption-text">A time-frequency representation. One dimension is indexed by time, another is
index by frequency, and values in the matrix represent the energy of the sound
at that partifular time and frequency.</span><a class="headerlink" href="#tf-repr" title="Permalink to this image">¶</a></p>
</div>
<p>A Time-Frequency <span id="id3">[<a class="reference internal" href="../zzz_refs.html#id4">SI11</a>]</span> representation is a 2 dimensional
matrix that represents the frequency contents of an audio signal over time.
There are many types of time-frequency representations out in the world, but we
will only discuss those that are most frequently used for source separation here.</p>
<p>We call a specific entry in this matrix a TF bin. We can visualize a
TF Representation using a heatmap, which has time along the x-axis and
frequency along the y-axis. Each TF bin in the heatmap represents the amplitude
of the signal at that particular time and frequency. Some heatmaps have a colorbar
alongside them that shows which colors indicate high amplitude values and which
colors indicate low amplitude values. If there is no color bar, it is usually
safe to assume that brighter colors indicate higher amplitudes than darker colors.</p>
<p>Time-frequency representations are the most common types of representations used
in source separation approaches. Below we will outline some of the most popular
and fundamental time-frequency representations.</p>
<div class="section" id="short-time-fourier-transform-stft">
<h4>Short-time Fourier Transform (STFT)<a class="headerlink" href="#short-time-fourier-transform-stft" title="Permalink to this headline">¶</a></h4>
<div class="figure align-default" id="stft-process">
<img alt="Diagram depicting how a short-time Fourier transform is computed." src="../_images/stft_process.png" />
<p class="caption"><span class="caption-text">The process of computing a short-time Fourier transform of a waveform. Imaged used
courtesy of Bryan Pardo.</span><a class="headerlink" href="#stft-process" title="Permalink to this image">¶</a></p>
</div>
<p>Many of the time-frequency representations that we will see in this tutorial
start out as a Short-time Fourier Transform or STFT. An STFT is calculated
from a waveform representation by computing a
<a class="reference external" href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">discrete Fourier transform</a>
(DFT) of a small, moving window<a class="footnote-reference brackets" href="#fn1" id="id4">2</a> across the duration of the window. The location
of each entry in an STFT determines its time (x-axis) and frequency (y-axis). The
absolute value of a TF bin <span class="math notranslate nohighlight">\(|X(t, f)|\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and frequency <span class="math notranslate nohighlight">\(f\)</span> determines the
amount of energy heard from frequency <span class="math notranslate nohighlight">\(f\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>Importantly, each bin in our STFT is <em>complex</em>, meaning each entry contains both
a magnitude component and a phase component. Both components are needed to convert
an STFT matrix back to a waveform so that we may hear it.</p>
<p>The STFT is invertable, meaning that a complex valued STFT can be converted back
to a waveform. This is called the inverse Short-time Fourier Transform or iSTFT.</p>
<p>Here are some important parameters to consider when computing an STFT:</p>
<div class="section" id="window-types">
<h5>Window Types<a class="headerlink" href="#window-types" title="Permalink to this headline">¶</a></h5>
<div class="figure align-default" id="id5">
<img alt="Six commonly used window types for calculating an STFT." src="../_images/window_types.png" />
<p class="caption"><span class="caption-text">Six commonly used window types for calculating an STFT.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>The window type determines the shape of the short-time window that will segment
the audio into short segments before applying the DFT. The shape of this window
will affect which frequencies get emphasized or attenuated in the DFT. There
are many types of window functions, in <code class="xref std std-numref docutils literal notranslate"><span class="pre">window_types</span></code>, we show the most
common ones when calculating an STFT for source separation. For more information
on other types of windows and their frequency response, please see <code class="docutils literal notranslate"><span class="pre">scipy.signal</span></code>’s
documentation <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/signal.windows.html">here</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We have informally noticed that our models acheive the best performance using
the <code class="docutils literal notranslate"><span class="pre">sqrt_hann</span></code> window, shown above.</p>
</div>
</div>
<div class="section" id="window-length">
<h5>Window Length<a class="headerlink" href="#window-length" title="Permalink to this headline">¶</a></h5>
<div class="figure align-default" id="window-lens">
<img alt="Trade off between time and frequency resolution for different window lengths." src="../_images/win_len.png" />
<p class="caption"><span class="caption-text">Trade off between time and frequency resolution for different window lengths.
In the simplified case, the number of frequency bins is determined by the window
length.
Using the same signal, we compute the STFT once with window length 512, and
again with window length 1024. The white space is where there is no STFT data,
which is left in to intentionally show how time and frequency interplay with
the window length.</span><a class="headerlink" href="#window-lens" title="Permalink to this image">¶</a></p>
</div>
<p>The window length determines how many samples are included in each short-time
window. Due to how the DFT is computed, this parameter also determines the
resolution of the frequency axis of the STFT. The longer the window, the higher
the frequency resolution and vice versa, as is visible in <code class="xref std std-numref docutils literal notranslate"><span class="pre">window_lens</span></code>.</p>
</div>
<div class="section" id="hop-length">
<h5>Hop Length<a class="headerlink" href="#hop-length" title="Permalink to this headline">¶</a></h5>
<div class="figure align-default" id="hop-lens">
<img alt="The hope length determines the distance (in samples) between adjacent short-time windows." src="../_images/hop_len.png" />
<p class="caption"><span class="caption-text">The hop length determines the distance (in samples) between adjacent short-time
windows. An STFT is computed twice on the same signal; the smaller the hop length
the more times a particular segment of the audio signal is represented in the
STFT.</span><a class="headerlink" href="#hop-lens" title="Permalink to this image">¶</a></p>
</div>
<p>The hop length determines the distance, in samples, between any two adjacent
short-time windows. In the image shown in <code class="xref std std-numref docutils literal notranslate"><span class="pre">hop_lens</span></code> shows how the hop
length can elongate or shorten the time axis on an STFT depending on how it is
set.</p>
</div>
<div class="section" id="overlap-add">
<h5>Overlap-Add<a class="headerlink" href="#overlap-add" title="Permalink to this headline">¶</a></h5>
<p>Many parameter settings for window type, window length, and hop length might
not reconstruct the original signal perfectly when converting a signal to an STFT
and back to a waveform. However, a certain set of parameters
is mathematically guaranteed to perfectly reconstruct any signal. These parameter sets
are called Constant Overlap-Add (COLA) because when applying successive windows,
they add up to a <a class="reference external" href="https://www.dsprelated.com/freebooks/sasp/Constant_Overlap_Add_COLA_Cases.html">constant value</a>.</p>
<p>In general, we tend to use a hop length that is half of the window length, but
there are <a class="reference external" href="https://gist.github.com/endolith/c5b39ab78f1910e99cadaced9b839fc1">many documented COLA settings</a>,
and an <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.check_COLA.html">easy way to check if a tuple of settings is COLA</a>.</p>
</div>
</div>
<div class="section" id="magnitude-power-and-log-spectrograms">
<h4>Magnitude, Power, and Log Spectrograms<a class="headerlink" href="#magnitude-power-and-log-spectrograms" title="Permalink to this headline">¶</a></h4>
<div class="figure align-default" id="spectrograms">
<img alt="Four types of spectrograms with different scaling for function." src="../_images/ylinear_specs.png" />
<p class="caption"><span class="caption-text">A visual comparison of the four types of spectrograms discussed in this section.
Each has a different scaling function applied to the loudness, which is reflected
in the colors of the heatmap plots.</span><a class="headerlink" href="#spectrograms" title="Permalink to this image">¶</a></p>
</div>
<p>As we will touch on later in this tutorial, it is hard to model the
phase of a signal. Therefore most source separation approaches only operate on
the some variant of the spectrogram that does not explicitly represent phase in
each TF bin. A visualization of four types of spectrograms is shown
in <code class="xref std std-numref docutils literal notranslate"><span class="pre">spectrograms</span></code>.</p>
<p><strong>Magnitude Spectrogram</strong>
For a complex-valued STFT, <span class="math notranslate nohighlight">\(X \in \mathbb{C}^{T \times F}\)</span>, the Magnitude
Spectrogram is calculated by taking the absolute value of each element in the
STFT, <span class="math notranslate nohighlight">\(|X| \in \mathbb{R}^{T \times F}\)</span>.</p>
<p><strong>Power Spectrogram</strong>
For a complex-valued STFT, <span class="math notranslate nohighlight">\(X \in \mathbb{C}^{T \times F}\)</span>, the Power
Spectrogram is calculated by squaring  each element in the
STFT, <span class="math notranslate nohighlight">\(|X|^2 \in \mathbb{R}^{T \times F}\)</span>.</p>
<p><strong>Log Spectrogram</strong>
Human hearing is logarithmic with regards to amplitude.
For a complex-valued STFT, <span class="math notranslate nohighlight">\(X \in \mathbb{C}^{T \times F}\)</span>, the Log
Spectrogram is calculated taking the log of the absolute value of each element in the
STFT, <span class="math notranslate nohighlight">\(\log{|X|} \in \mathbb{R}^{T \times F}\)</span>.</p>
<p><strong>Log Power Spectrogram</strong>
For a complex-valued STFT, <span class="math notranslate nohighlight">\(X \in \mathbb{C}^{T \times F}\)</span>, the Log
Spectrogram is calculated taking the log of the square of each element in the
STFT, <span class="math notranslate nohighlight">\(\log{|X|^2} \in \mathbb{R}^{T \times F}\)</span>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Even though it is hard to visualize the detail in a magnitude or power spectrogram,
some source separation algorithms work completely fine on these representations, while
some need log spectrograms. Make sure to set your spectrograms correctly!</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A note on terminology: while researchers might loosely interchange “STFT” and
“spectrogram”, the term “spectrogram” is mostly used to describe a TF Representation
that does not have any explicit phase representation. As such “spectrogram” might
refer to a Magnitude Spectrogram, Power Spectrogram, Log Spectrogram, Mel Spectrogram,
Log Mel Spectrogram, or similar. Use context clues to determine which representation
is being discussed when possible.</p>
</div>
</div>
<div class="section" id="mel-spaced-spectrograms">
<h4>Mel-spaced Spectrograms<a class="headerlink" href="#mel-spaced-spectrograms" title="Permalink to this headline">¶</a></h4>
<div class="figure align-default" id="mel-spectrograms">
<a class="reference internal image-reference" href="../_images/ymel_specs.png"><img alt="A visual comparison of linear-scaled vs mel-spaced y axies." src="../_images/ymel_specs.png" style="width: 280.0px; height: 350.0px;" /></a>
<p class="caption"><span class="caption-text">A visual comparison of linear-scaled vs mel-spaced y axies.
Lower frequencies have a larger representation in a mel-spaced spectrogram.</span><a class="headerlink" href="#mel-spectrograms" title="Permalink to this image">¶</a></p>
</div>
<p>Human hearing is also logarithmic with regards to frequencies. The
<a class="reference external" href="https://en.wikipedia.org/wiki/Mel_scale">Mel scale</a> approximates this property
and is a quick way to make the frequency axis of a spectrogram
quasi-logarithmic<a class="footnote-reference brackets" href="#fn3" id="id6">3</a>. This is also commonly used to reduce the computational
load on deep learning-based approaches, because the number of Mel-spaced frequency
bins is often lower then the number of linearly-spaced frequency bins.</p>
<p>A visual comparison is shown in <code class="xref std std-numref docutils literal notranslate"><span class="pre">mel_spectrograms</span></code>. Notice the how the
y-axis in the Mel-spaced spectrogram is quasi-logarithmic, squishing higher frequencies
and leaving more space for lower frequencies.</p>
</div>
</div>
<div class="section" id="other-representations">
<h3>Other Representations<a class="headerlink" href="#other-representations" title="Permalink to this headline">¶</a></h3>
<p>A few other representations have been explored in the literature. We provide references
for a few below:</p>
<ul class="simple">
<li><p>Constant-Q Transform (CQT)</p>
<ul>
<li><p>General Calculation: <span id="id7">[<a class="reference internal" href="../zzz_refs.html#id9">Bro91</a>, <a class="reference internal" href="../zzz_refs.html#id10">BP92</a>]</span></p></li>
<li><p>Use in source separation: <span id="id8">[<a class="reference internal" href="../zzz_refs.html#id12">FBR12</a>, <a class="reference internal" href="../zzz_refs.html#id14">GSD12</a>, <a class="reference internal" href="../zzz_refs.html#id13">JFB+11</a>, <a class="reference internal" href="../zzz_refs.html#id11">RP11</a>, <a class="reference internal" href="../zzz_refs.html#id15">SLL+19</a>]</span></p></li>
</ul>
</li>
<li><p>Common Fate Transform (CFT) <span id="id9">[<a class="reference internal" href="../zzz_refs.html#id6">StoterLB+16</a>]</span></p></li>
<li><p>Multi-resolution Common Fate Transform (MCFT) <span id="id10">[<a class="reference internal" href="../zzz_refs.html#id7">PP18</a>]</span></p></li>
<li><p>2-Dimensional Fourier Transform (2DFT) <span id="id11">[<a class="reference internal" href="../zzz_refs.html#id8">SPP17</a>]</span></p></li>
<li><p>Per Channel Energy Normalization (PCEN) <span id="id12">[<a class="reference internal" href="../zzz_refs.html#id5">LSC+18</a>]</span></p></li>
</ul>
</div>
</div>
<div class="section" id="output-representations">
<h2>Output Representations<a class="headerlink" href="#output-representations" title="Permalink to this headline">¶</a></h2>
<p>Ultimately, all source separation algorithms must be able to convert the audio
that they processed back to a waveform. While some algorithms output waveforms
directly, many algorithms output masks, which will be covered in more detail in
the next sections. The masks get applied to the original mixture spectrogram,
and that result is converted back to a waveform.</p>
<p>One thing to note is that if I have a waveform estimate for Source <span class="math notranslate nohighlight">\(i\)</span> from my
mixture, then it is easy to calculate what the mixture sounds like <em>without</em>
Source <span class="math notranslate nohighlight">\(i\)</span> present. Simply element-wise subtract the source waveform from the
mixture waveform.</p>
</div>
<div class="section" id="which-is-better-inputting-a-waveform-or-a-time-frequency-representation">
<h2>Which is Better? Inputting a Waveform or a Time-Frequency Representation?<a class="headerlink" href="#which-is-better-inputting-a-waveform-or-a-time-frequency-representation" title="Permalink to this headline">¶</a></h2>
<p>Very few non-deep learning source separation approaches operate directly
on waveforms, so we will restrict our answer to deep learning methods, which
have dominated the field for the past few years. Even still, the answer is that
it depends!</p>
<p>While this may change in the coming years, in general, determining which
approach is best a <em>very hard</em> problem, as we will see in the
<a class="reference internal" href="evaluation.html#evaluation"><span class="std std-ref">Evaluation</span></a> section.</p>
<p>In recent years, many state-of-the-art systems for music separation
have used both spectrogram and waveforms as input. However, research in
speech separation has mostly converged upon using the waveform as input. This
may be a sign of things to come for music separation; we’ll have to wait and
see!</p>
<p>By the end of this tutorial, we hope that you will be able to make an educated
decision about which type of source system is right for your goals.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="fn2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Named after <a href="https://en.wikipedia.org/wiki/Heinrich_Hertz">Heinrich Hertz</a>,
who proved the existence of electromagnetic waves.</p>
</dd>
<dt class="label" id="fn1"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>This window is where the term “Short-time” comes from in the name
“Short-time Fourier Transform”.</p>
</dd>
<dt class="label" id="fn3"><span class="brackets"><a class="fn-backref" href="#id6">3</a></span></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">librosa</span></code>, arguably the most commonly used python library for
MIR work, has two ways to convert from Hz to Mel, which are slightly
different: <a href="https://librosa.org/doc/latest/_modules/librosa/core/convert.html#hz_to_mel">see here.</a></p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "source-separation/tutorial",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./basics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Alexandre Defossez, Haohe Liu, Woosung Choi, Ethan Manilow, Prem Seetharaman, Justin Salamon<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-180111316-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>