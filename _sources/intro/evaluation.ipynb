{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5895aa6",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "It is challenging to measure the quality of a source separation method.\n",
    "The [previous tutorial](https://source-separation.github.io/tutorial/basics/evaluation.html\n",
    ") introduces two main categories (i.e., objective and subjective) for measuring the quality of a source separation approach in detail.\n",
    "This section summarizes each evaluation method as follows.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Objective methods measure the dissimilarity (or similarity) between ground-truth targets and the estimated signals.\n",
    "Source-to-Distortion Ratio (SDR), Source-to-Interference Ratio (SIR), and Source-to-Artifact Ratio (SAR), proposed in {cite}`vincent2006performance`, are the most widely used objective evaluation methods.\n",
    "The MDX challenge uses SDR as an evaluation metric.\n",
    "\n",
    "### Source-to-Distortion Ratio (SDR)\n",
    "\n",
    "The Source-to-Distortion Ratio (SDR) measures the ratio of the energy of a source to the energy of the distortion.\n",
    "\n",
    "{cite}`vincent2006performance` assumed that an estimate of a source $\\hat{s}_i$ consists of four components,\n",
    "\n",
    "$$\n",
    "\\hat{s}_i = s_{\\text{target}} + e_{\\text{interf}} + e_{\\text{noise}} + e_{\\text{artif}},\n",
    "$$\n",
    "\n",
    "where $s_{\\text{target}}$ is the true source, and $e_{\\text{interf}}$, $e_{\\text{noise}}$, and\n",
    "$e_{\\text{artif}}$ are error terms for interference, noise, and added artifacts,\n",
    "respectively. \n",
    "\n",
    "Each term must be calculated respectively to obtain SIR or SNR.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "s_\\text{distortion} & = e_{\\text{interf}} + e_{\\text{noise}} + e_{\\text{artif}} \\\\\n",
    "& = \\hat{s}_i - s_{\\text{target}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, SDR does not require individual terms because *distortion* is defined as the sum $e_{\\text{interf}} + e_{\\text{noise}} + e_{\\text{artif}}$, which is identical to $\\hat{s}_i - s_{\\text{target}}$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{SDR} & := 10 \\log_{10} \\left( \\frac{\\| s_{\\text{target}} \\|^2}{ \\| e_{\\text{interf}} + e_{\\text{noise}} + e_{\\text{artif}} \\|^2} \\right) \\\\\n",
    "& = 10 \\log_{10} \\left( \\frac{\\| s_{\\text{target}} \\|^2}{ \\| \\hat{s}_i - s_{\\text{target}} \\|^2} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If a result is ideal (i.e., $\\hat{s}_i = s_{\\text{target}} $), it goes positive infinity.\n",
    "If a result is far from the ground truth, then the energy of the distortion goes large, which makes SDR lower.\n",
    "SDR is usually considered to be an overall measure of how good a source sounds.\n",
    "If a paper only reports one number for estimated quality, it is usually SDR.\n",
    "\n",
    "\n",
    "#### Global vs Framewise Computation\n",
    "\n",
    "The MDX challenge uses *global SDR*, which computes the metric on the entire song.\n",
    "Some existing evaluation tools such as ```museval``` {cite}`sisec18` compute the metric on shorter frames (or windows) and take the average as a result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dbb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aa5d551",
   "metadata": {},
   "source": [
    "### Source-to-Artifact Ratio (SAR)\n",
    "\n",
    "$$\n",
    "\\text{SAR} := 10 \\log_{10} \\left( \\frac{\\| s_{\\text{target}} + e_{\\text{interf}} + e_{\\text{noise}} \\|^2}{ \\| e_{\\text{artif}} \\|^2} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "The Source-to-Artifact Ratio (SAR) measures the ratio of the energy of a source to the energy of the artifacts. Artifacts are unwanted noises, usually generated from models (e.g. neural networks, algorithms), not other sources.\n",
    "\n",
    "### Source-to-Interference Ratio (SIR)\n",
    "\n",
    "$$\n",
    "\\text{SIR} := 10 \\log_{10} \\left( \\frac{\\| s_{\\text{target}} \\|^2}{ \\| e_{\\text{interf}} \\|^2} \\right)\n",
    "$$\n",
    "\n",
    "The Source-to-Interference Ratio (SIR) measures the ratio of the energy of a source to the energy of the interferences. If we can hear sounds clearer from the other sources in the estimated source, SIR would be lower. It is similar to the concept of [\"bleed\", or \"leakage\"](https://en.wikipedia.org/wiki/Spill_(audio)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c42e2",
   "metadata": {},
   "source": [
    "## Subjective\n",
    "\n",
    "Although many researchers have used the SDR family for their quantitative evaluation, SDR does not tell everything. As discussed in the [previous tutorial](https://source-separation.github.io/tutorial/basics/evaluation.html), there are some cases where the SDR results are quite different from human perception.\n",
    "\n",
    "Subjective measures, where human evaluators listen to samples and score, might be alternative if participants are sufficiently large.\n",
    "Although this usually requires a lot of time/money and it is hard to make realiable results, it provides evaluation reflecting human perception."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
